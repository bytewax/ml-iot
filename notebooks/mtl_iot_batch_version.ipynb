{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Batch processing of Air Quality Data for Anomaly detection\n",
        "\n",
        "This Jupyter notebook demonstrates the application of machine learning techniques for the batch processing and anomaly detection of air quality data. The motivation behind this notebook is to provide an automated solution for cleaning, imputing missing values, and detecting anomalies in air quality data sets. Such processes are critical for environmental monitoring and ensuring the reliability of data used in further analysis or reporting.\n",
        "\n",
        "### Pipeline Overview\n",
        "The processing pipeline can be visualized as follows:\n",
        "\n",
        "```bash\n",
        "Data Retrieval -> Data Serialization -> Data Deserialization -> Data Imputation -> Anomaly Detection\n",
        "```\n",
        "\n",
        "### About the Data\n",
        "\n",
        "We obtained a sample of the data from the [Purple AIR API](https://community.purpleair.com/t/making-api-calls-with-the-purpleair-api/180)\n",
        "\n",
        "In this notebook you can find code to clean the data, perform missing values imputation, and detect anomalies using River."
      ],
      "metadata": {
        "id": "7p-RpoZqpxYp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkvR8L39udw-",
        "outputId": "1d76e4c9-890b-4741-b83d-09af44c06282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install bytewax==0.19 python-dotenv scipy==1.13.0 kafka-python==2.0.2 --q\n",
        "!pip install pandas==2.0.3\n",
        "!pip install scikit-learn==1.4.2 --q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detailed Code Walkthrough\n",
        "Below is a breakdown of each component of the pipeline, accompanied by Python code snippets and their explanations.\n",
        "\n",
        "Below are the imports we will use.\n",
        "\n"
      ],
      "metadata": {
        "id": "cQiy2mbqIE-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timezone\n",
        "from sklearn.impute import KNNImputer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n"
      ],
      "metadata": {
        "id": "VW1OWHW_Iuq_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Retrieval\n",
        "Here, the air quality data is fetched from a provided URL."
      ],
      "metadata": {
        "id": "nk-AX4MjIJ2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url = 'https://raw.githubusercontent.com/bytewax/ml-iot/main/data.json'\n",
        "\n",
        "resp = requests.get(url)\n",
        "data = json.loads(resp.text)\n"
      ],
      "metadata": {
        "id": "BjZIHP06n6k3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the data"
      ],
      "metadata": {
        "id": "Zq7UNtplIeFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SDjkH9UoSSl",
        "outputId": "3bb15436-65da-4deb-b84a-65161fb39633"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['api_version', 'time_stamp', 'data_time_stamp', 'max_age', 'firmware_default_version', 'fields', 'data'])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['fields']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxCyaIRHIaic",
        "outputId": "98c15a3e-1405-46d7-c392-d92114713bf6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sensor_index',\n",
              " 'date_created',\n",
              " 'rssi',\n",
              " 'uptime',\n",
              " 'latitude',\n",
              " 'longitude',\n",
              " 'humidity',\n",
              " 'temperature',\n",
              " 'pressure',\n",
              " 'pm1.0',\n",
              " 'pm2.5_alt',\n",
              " 'pm10.0',\n",
              " 'pm1.0_cf_1',\n",
              " 'pm2.5_atm',\n",
              " 'pm2.5_cf_1',\n",
              " 'pm10.0_cf_1']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['data'][0:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4mh6XUbIcRO",
        "outputId": "5309f27f-66d3-461c-b732-f2107157fd7f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[53,\n",
              "  1454548891,\n",
              "  -50,\n",
              "  10183,\n",
              "  40.246742,\n",
              "  -111.7048,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  0.0,\n",
              "  2.1,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [77,\n",
              "  1456896339,\n",
              "  -58,\n",
              "  2320,\n",
              "  40.750816,\n",
              "  -111.82529,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  15.5,\n",
              "  14.5,\n",
              "  15.9,\n",
              "  15.5,\n",
              "  15.8,\n",
              "  15.8,\n",
              "  15.9]]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the structure of the data, we will first serialize the information so that each record consists of dictionaries with the names of the fields and the corresponding values. In this way, we will 'flatten' the JSON object.\n",
        "\n",
        "We define a `deserialize` function. This function takes structured data and converts it into a serialized byte format, preparing it for further processing steps like transmission over a network.\n",
        "\n"
      ],
      "metadata": {
        "id": "SLsO_hp5IgkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def serialize(data):\n",
        "    \"\"\"\n",
        "    This function serializes the data by converting it\n",
        "    to a JSON string and then encoding it to bytes.\n",
        "\n",
        "    Args:\n",
        "    data: A dictionary containing the data to be serialized.\n",
        "\n",
        "    Returns:\n",
        "    A list of serialized data in bytes format.\n",
        "    \"\"\"\n",
        "    headers = data['fields']\n",
        "    serialized_data = []\n",
        "\n",
        "    for entry in data['data']:\n",
        "        try:\n",
        "            # Create a dictionary for each entry, matching fields with values\n",
        "            entry_data = {headers[i]: entry[i] for i in range(len(headers))}\n",
        "            # Convert the dictionary to a JSON string and then encode it to bytes\n",
        "            entry_bytes = json.dumps(entry_data).encode('utf-8')\n",
        "            serialized_data.append(entry_bytes)\n",
        "        except IndexError:\n",
        "            # This block catches cases where the entry might not have all the fields\n",
        "            print(\"IndexError with entry:\", entry)\n",
        "            continue\n",
        "\n",
        "    return serialized_data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pkEzRUBuIQ_I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we serialized the data, we will conver the byte data back into a usable dictionary format.\n",
        "\n",
        "This function decodes byte data back into a structured dictionary format, including converting timestamps from epoch to Python datetime objects and ensuring numerical data types are correct for analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "DKtuMIhUI0Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deserialize(byte_objects_list):\n",
        "    \"\"\"\n",
        "    This function deserializes the data by decoding the bytes\n",
        "    it converts epoch time to a datetime object and converts\n",
        "    \"pm2.5_cf_1\" to a float.\n",
        "\n",
        "    Args:\n",
        "    byte_objects_list: A list of byte objects to be deserialized.\n",
        "\n",
        "    Returns:\n",
        "    A list of dictionaries containing the deserialized data.\n",
        "    \"\"\"\n",
        "    results = []  # List to hold the processed sensor data\n",
        "    for byte_object in byte_objects_list:\n",
        "        if byte_object:  # Check if byte_object is not empty\n",
        "            sensor_data = json.loads(byte_object.decode('utf-8'))  # Decode and load JSON from bytes\n",
        "\n",
        "            # Convert \"pm2.5_cf_1\" to a float, check if the value exists and is not None\n",
        "            if 'pm2.5_cf_1' in sensor_data and sensor_data['pm2.5_cf_1'] is not None:\n",
        "                sensor_data['pm2.5_cf_1'] = float(sensor_data['pm2.5_cf_1'])\n",
        "\n",
        "            # Convert \"date_created\" from Unix epoch time to a datetime object, check if the value exists\n",
        "            if 'date_created' in sensor_data and sensor_data['date_created'] is not None:\n",
        "                sensor_data['date_created'] = datetime.fromtimestamp(sensor_data['date_created'], tz=timezone.utc)\n",
        "\n",
        "            results.append(sensor_data)  # Add the processed data to the results list\n",
        "\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "YP_89djiIVwS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have completed this step, it is now needed to impute values. This method uses the KNN imputation technique to fill in missing or null values in the data, which is essential for maintaining the integrity of subsequent analyses."
      ],
      "metadata": {
        "id": "XMXeuG2mI5eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def impute_data_with_knn(deserialized_data):\n",
        "    \"\"\"\n",
        "    Takes a list of dictionaries from deserialized data, converts it into a DataFrame,\n",
        "    performs KNN imputation, and converts it back into a list of dictionaries.\n",
        "\n",
        "    Args:\n",
        "    deserialized_data: A list of dictionaries containing sensor data.\n",
        "\n",
        "    Returns:\n",
        "    A list of dictionaries with imputed data.\n",
        "    \"\"\"\n",
        "    # Convert list of dictionaries to DataFrame\n",
        "    df = pd.DataFrame(deserialized_data)\n",
        "\n",
        "    # Ensure all numeric columns are in appropriate data types\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'object':\n",
        "            try:\n",
        "                df[column] = pd.to_numeric(df[column])\n",
        "            except ValueError:\n",
        "                continue  # Keep non-convertible columns as object if needed\n",
        "\n",
        "    # Apply KNN imputer to numeric columns\n",
        "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "    imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
        "    imputed_array = imputer.fit_transform(df[numeric_columns])\n",
        "\n",
        "    # Update numeric columns in DataFrame with imputed values\n",
        "    df[numeric_columns] = imputed_array\n",
        "\n",
        "    # Convert DataFrame back to a list of dictionaries\n",
        "    imputed_data = df.to_dict(orient='records')\n",
        "\n",
        "    return imputed_data"
      ],
      "metadata": {
        "id": "PowuhtNyI57Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will define an anomaly detector class.\n",
        "\n",
        "\n",
        "The Isolation Forest is a good choice for batch anomaly detection due to its effectiveness with multidimensional data and its capability of identifying anomalies without needing a target label. Here's how:\n",
        "\n",
        "1. Initialize the Isolation Forest: The detector is initialized with parameters such as the number of trees (n_estimators), the sample size (max_samples), and the contamination factor which represents the proportion of outliers expected in the data.\n",
        "2. Fit the Model: Since we are assuming batch processing, the model can be fitted on a predefined dataset. This dataset should ideally represent typical \"normal\" data to help the model learn the structure of non-anomalous data.\n",
        "3. Predict and Score Anomalies: After the model is fitted, it can predict new data points as anomalies based on the isolation properties learned during training. The `score_samples` function gives a raw anomaly score, which can be used to determine how anomalous a data point is.\n"
      ],
      "metadata": {
        "id": "dmYppULVJEuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnomalyDetector:\n",
        "    \"\"\"\n",
        "    Anomaly detector using Isolation Forest from scikit-learn\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_estimators=100, max_samples='auto', contamination=0.01, random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize the anomaly detector with parameters suitable for Isolation Forest\n",
        "        \"\"\"\n",
        "        self.detector = IsolationForest(\n",
        "            n_estimators=n_estimators,\n",
        "            max_samples=max_samples,\n",
        "            contamination=contamination,\n",
        "            random_state=random_state\n",
        "        )\n",
        "\n",
        "    def fit(self, data):\n",
        "        \"\"\"\n",
        "        Fit the Isolation Forest model with data\n",
        "        \"\"\"\n",
        "        self.detector.fit(data)\n",
        "\n",
        "    def predict(self, data):\n",
        "        \"\"\"\n",
        "        Predict data using the fitted model and tag entries as anomalies\n",
        "        \"\"\"\n",
        "        # -1 for anomalies, 1 for normal\n",
        "        predictions = self.detector.predict(data)\n",
        "        scores = self.detector.score_samples(data)\n",
        "        return predictions, scores"
      ],
      "metadata": {
        "id": "s3_xJeS3I7rY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's bring the pieces together."
      ],
      "metadata": {
        "id": "QpMyldR4JJiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Begin data processing\n",
        "# Serialize the data to bytes\n",
        "serialized_entries = serialize(data)\n",
        "# Deserialize the data and transform epoch\n",
        "deserialized_data = deserialize(serialized_entries)\n"
      ],
      "metadata": {
        "id": "kNgWXpSAzHji"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(deserialized_data).isna().sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bddYWgWGBHvl",
        "outputId": "3c4f26e7-94f7-4773-f006-832540f326b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sensor_index       0\n",
              "date_created       0\n",
              "rssi               1\n",
              "uptime             1\n",
              "latitude          54\n",
              "longitude         54\n",
              "humidity         965\n",
              "temperature      965\n",
              "pressure        1023\n",
              "pm1.0             31\n",
              "pm2.5_alt         31\n",
              "pm10.0            31\n",
              "pm1.0_cf_1        31\n",
              "pm2.5_atm         31\n",
              "pm2.5_cf_1        31\n",
              "pm10.0_cf_1       31\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform KNN imputation on deserialized data\n",
        "imputed_data = impute_data_with_knn(deserialized_data)\n"
      ],
      "metadata": {
        "id": "9j-wkZ4ioMef"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(imputed_data).isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJVgNLtuBG9Z",
        "outputId": "56275d30-a2f5-4ad0-b4c0-a64ccc51efc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sensor_index    0\n",
              "date_created    0\n",
              "rssi            0\n",
              "uptime          0\n",
              "latitude        0\n",
              "longitude       0\n",
              "humidity        0\n",
              "temperature     0\n",
              "pressure        0\n",
              "pm1.0           0\n",
              "pm2.5_alt       0\n",
              "pm10.0          0\n",
              "pm1.0_cf_1      0\n",
              "pm2.5_atm       0\n",
              "pm2.5_cf_1      0\n",
              "pm10.0_cf_1     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can identify outliers after applying the model as seen below."
      ],
      "metadata": {
        "id": "qCWyRpl8xQGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare your data (assuming imputed_data is a DataFrame ready for input)\n",
        "detector = AnomalyDetector()\n",
        "\n",
        "# You must convert the list of dictionaries to a DataFrame if not already done\n",
        "df = pd.DataFrame(imputed_data)\n",
        "\n",
        "# Select only the numeric columns for anomaly detection\n",
        "numeric_columns = df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Fit the model with numeric data\n",
        "detector.fit(numeric_columns)\n",
        "\n",
        "# Predict anomalies on the same or new data\n",
        "predictions, scores = detector.predict(numeric_columns)"
      ],
      "metadata": {
        "id": "y01BBE2XyhGH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add predictions and scores back to the DataFrame for review or further processing\n",
        "df['anomaly'] = predictions\n",
        "df['score'] = scores\n",
        "\n",
        "# Print or process anomalies\n",
        "anomalies = df[df['anomaly'] == -1]\n",
        "anomalies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "mmfNOv3Soh12",
        "outputId": "219ff3a2-381e-41c8-a1d3-332a56d4f180"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       sensor_index              date_created  rssi   uptime   latitude  \\\n",
              "100          1970.0 2017-07-11 18:58:30+00:00 -76.0  17419.0  33.998270   \n",
              "127          2334.0 2017-07-31 18:03:42+00:00 -55.0    231.0  41.045155   \n",
              "1116        13247.0 2018-07-15 18:44:55+00:00 -76.0     74.0  12.448359   \n",
              "1161        13907.0 2018-08-01 20:40:19+00:00 -79.0  26054.0  37.851050   \n",
              "1748        19383.0 2018-11-16 22:24:14+00:00 -72.0  62256.0  37.328760   \n",
              "...             ...                       ...   ...      ...        ...   \n",
              "25038      206631.0 2023-12-18 19:42:28+00:00 -73.0   7628.0  21.075000   \n",
              "25076      207267.0 2023-12-20 20:14:46+00:00 -73.0   7628.0  21.075190   \n",
              "25190      209433.0 2024-01-04 16:05:37+00:00 -59.0  41566.0  13.705800   \n",
              "25322      211941.0 2024-01-26 19:39:40+00:00 -63.0    336.0  59.360455   \n",
              "25550      221047.0 2024-04-08 21:41:43+00:00 -41.0    157.0  37.638870   \n",
              "\n",
              "        longitude  humidity  temperature  pressure   pm1.0  pm2.5_alt  pm10.0  \\\n",
              "100   -118.437546      34.0         86.0   1013.58  2759.8        0.0  2759.8   \n",
              "127   -111.985910      70.0         39.0    862.87   108.9       53.6   133.7   \n",
              "1116    75.694170      70.0         86.0    887.86   123.2      153.2   185.6   \n",
              "1161  -122.271750      27.0         92.0   1013.48  3330.9        0.0  3330.9   \n",
              "1748  -121.897870      36.0         86.0   1012.87   155.4      149.4   310.6   \n",
              "...           ...       ...          ...       ...     ...        ...     ...   \n",
              "25038  105.809070      60.0         85.0   1013.73    34.3       55.0    95.3   \n",
              "25076  105.808510      59.0         86.0   1013.89    34.1       49.9    86.4   \n",
              "25190  100.539000      50.0         93.0   1001.68    44.2       54.9    72.7   \n",
              "25322   17.990343      30.0         78.0   1015.15   112.0      160.5   192.2   \n",
              "25550 -122.055730      30.0         91.0   1014.84   245.5      220.1   430.5   \n",
              "\n",
              "       pm1.0_cf_1  pm2.5_atm  pm2.5_cf_1  pm10.0_cf_1  anomaly     score  \n",
              "100        4139.3     2759.8      4139.3       4139.3       -1 -0.743272  \n",
              "127         159.0      132.4       185.8        186.5       -1 -0.737338  \n",
              "1116        185.8      176.0       265.1        279.4       -1 -0.760949  \n",
              "1161       4997.0     3330.9      4997.0       4997.0       -1 -0.757399  \n",
              "1748        155.4      178.0       268.2        310.6       -1 -0.730170  \n",
              "...           ...        ...         ...          ...      ...       ...  \n",
              "25038        52.2       60.8        92.5        144.3       -1 -0.698228  \n",
              "25076        50.4       57.8        87.8        128.8       -1 -0.689231  \n",
              "25190        67.7       61.3        92.9        101.1       -1 -0.707000  \n",
              "25322       169.3      181.5       273.5        289.7       -1 -0.755836  \n",
              "25550       245.5      251.9       379.1        430.5       -1 -0.759713  \n",
              "\n",
              "[256 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f542eaa4-34d4-4c3a-8b9c-b3a24f23a1e5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sensor_index</th>\n",
              "      <th>date_created</th>\n",
              "      <th>rssi</th>\n",
              "      <th>uptime</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>humidity</th>\n",
              "      <th>temperature</th>\n",
              "      <th>pressure</th>\n",
              "      <th>pm1.0</th>\n",
              "      <th>pm2.5_alt</th>\n",
              "      <th>pm10.0</th>\n",
              "      <th>pm1.0_cf_1</th>\n",
              "      <th>pm2.5_atm</th>\n",
              "      <th>pm2.5_cf_1</th>\n",
              "      <th>pm10.0_cf_1</th>\n",
              "      <th>anomaly</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>1970.0</td>\n",
              "      <td>2017-07-11 18:58:30+00:00</td>\n",
              "      <td>-76.0</td>\n",
              "      <td>17419.0</td>\n",
              "      <td>33.998270</td>\n",
              "      <td>-118.437546</td>\n",
              "      <td>34.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1013.58</td>\n",
              "      <td>2759.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2759.8</td>\n",
              "      <td>4139.3</td>\n",
              "      <td>2759.8</td>\n",
              "      <td>4139.3</td>\n",
              "      <td>4139.3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.743272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>2334.0</td>\n",
              "      <td>2017-07-31 18:03:42+00:00</td>\n",
              "      <td>-55.0</td>\n",
              "      <td>231.0</td>\n",
              "      <td>41.045155</td>\n",
              "      <td>-111.985910</td>\n",
              "      <td>70.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>862.87</td>\n",
              "      <td>108.9</td>\n",
              "      <td>53.6</td>\n",
              "      <td>133.7</td>\n",
              "      <td>159.0</td>\n",
              "      <td>132.4</td>\n",
              "      <td>185.8</td>\n",
              "      <td>186.5</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.737338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1116</th>\n",
              "      <td>13247.0</td>\n",
              "      <td>2018-07-15 18:44:55+00:00</td>\n",
              "      <td>-76.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>12.448359</td>\n",
              "      <td>75.694170</td>\n",
              "      <td>70.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>887.86</td>\n",
              "      <td>123.2</td>\n",
              "      <td>153.2</td>\n",
              "      <td>185.6</td>\n",
              "      <td>185.8</td>\n",
              "      <td>176.0</td>\n",
              "      <td>265.1</td>\n",
              "      <td>279.4</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.760949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1161</th>\n",
              "      <td>13907.0</td>\n",
              "      <td>2018-08-01 20:40:19+00:00</td>\n",
              "      <td>-79.0</td>\n",
              "      <td>26054.0</td>\n",
              "      <td>37.851050</td>\n",
              "      <td>-122.271750</td>\n",
              "      <td>27.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1013.48</td>\n",
              "      <td>3330.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3330.9</td>\n",
              "      <td>4997.0</td>\n",
              "      <td>3330.9</td>\n",
              "      <td>4997.0</td>\n",
              "      <td>4997.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.757399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1748</th>\n",
              "      <td>19383.0</td>\n",
              "      <td>2018-11-16 22:24:14+00:00</td>\n",
              "      <td>-72.0</td>\n",
              "      <td>62256.0</td>\n",
              "      <td>37.328760</td>\n",
              "      <td>-121.897870</td>\n",
              "      <td>36.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1012.87</td>\n",
              "      <td>155.4</td>\n",
              "      <td>149.4</td>\n",
              "      <td>310.6</td>\n",
              "      <td>155.4</td>\n",
              "      <td>178.0</td>\n",
              "      <td>268.2</td>\n",
              "      <td>310.6</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.730170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25038</th>\n",
              "      <td>206631.0</td>\n",
              "      <td>2023-12-18 19:42:28+00:00</td>\n",
              "      <td>-73.0</td>\n",
              "      <td>7628.0</td>\n",
              "      <td>21.075000</td>\n",
              "      <td>105.809070</td>\n",
              "      <td>60.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>1013.73</td>\n",
              "      <td>34.3</td>\n",
              "      <td>55.0</td>\n",
              "      <td>95.3</td>\n",
              "      <td>52.2</td>\n",
              "      <td>60.8</td>\n",
              "      <td>92.5</td>\n",
              "      <td>144.3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.698228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25076</th>\n",
              "      <td>207267.0</td>\n",
              "      <td>2023-12-20 20:14:46+00:00</td>\n",
              "      <td>-73.0</td>\n",
              "      <td>7628.0</td>\n",
              "      <td>21.075190</td>\n",
              "      <td>105.808510</td>\n",
              "      <td>59.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1013.89</td>\n",
              "      <td>34.1</td>\n",
              "      <td>49.9</td>\n",
              "      <td>86.4</td>\n",
              "      <td>50.4</td>\n",
              "      <td>57.8</td>\n",
              "      <td>87.8</td>\n",
              "      <td>128.8</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.689231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25190</th>\n",
              "      <td>209433.0</td>\n",
              "      <td>2024-01-04 16:05:37+00:00</td>\n",
              "      <td>-59.0</td>\n",
              "      <td>41566.0</td>\n",
              "      <td>13.705800</td>\n",
              "      <td>100.539000</td>\n",
              "      <td>50.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>1001.68</td>\n",
              "      <td>44.2</td>\n",
              "      <td>54.9</td>\n",
              "      <td>72.7</td>\n",
              "      <td>67.7</td>\n",
              "      <td>61.3</td>\n",
              "      <td>92.9</td>\n",
              "      <td>101.1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.707000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25322</th>\n",
              "      <td>211941.0</td>\n",
              "      <td>2024-01-26 19:39:40+00:00</td>\n",
              "      <td>-63.0</td>\n",
              "      <td>336.0</td>\n",
              "      <td>59.360455</td>\n",
              "      <td>17.990343</td>\n",
              "      <td>30.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>1015.15</td>\n",
              "      <td>112.0</td>\n",
              "      <td>160.5</td>\n",
              "      <td>192.2</td>\n",
              "      <td>169.3</td>\n",
              "      <td>181.5</td>\n",
              "      <td>273.5</td>\n",
              "      <td>289.7</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.755836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25550</th>\n",
              "      <td>221047.0</td>\n",
              "      <td>2024-04-08 21:41:43+00:00</td>\n",
              "      <td>-41.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>37.638870</td>\n",
              "      <td>-122.055730</td>\n",
              "      <td>30.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1014.84</td>\n",
              "      <td>245.5</td>\n",
              "      <td>220.1</td>\n",
              "      <td>430.5</td>\n",
              "      <td>245.5</td>\n",
              "      <td>251.9</td>\n",
              "      <td>379.1</td>\n",
              "      <td>430.5</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.759713</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>256 rows × 18 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f542eaa4-34d4-4c3a-8b9c-b3a24f23a1e5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f542eaa4-34d4-4c3a-8b9c-b3a24f23a1e5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f542eaa4-34d4-4c3a-8b9c-b3a24f23a1e5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ffae3cd1-fb7e-45f5-9e37-ca63542bf5e0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ffae3cd1-fb7e-45f5-9e37-ca63542bf5e0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ffae3cd1-fb7e-45f5-9e37-ca63542bf5e0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "anomalies",
              "summary": "{\n  \"name\": \"anomalies\",\n  \"rows\": 256,\n  \"fields\": [\n    {\n      \"column\": \"sensor_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 49291.49057823271,\n        \"min\": 1970.0,\n        \"max\": 221047.0,\n        \"num_unique_values\": 256,\n        \"samples\": [\n          182171.0,\n          24519.0,\n          105224.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date_created\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2017-07-11 18:58:30+00:00\",\n        \"max\": \"2024-04-08 21:41:43+00:00\",\n        \"num_unique_values\": 256,\n        \"samples\": [\n          \"2023-05-25 18:21:17+00:00\",\n          \"2019-01-11 19:24:46+00:00\",\n          \"2021-04-27 19:23:20+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rssi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.75538909527435,\n        \"min\": -96.0,\n        \"max\": -16.0,\n        \"num_unique_values\": 66,\n        \"samples\": [\n          -93.0,\n          -40.0,\n          -76.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"uptime\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19778.418021100726,\n        \"min\": 4.0,\n        \"max\": 71429.0,\n        \"num_unique_values\": 239,\n        \"samples\": [\n          53286.0,\n          120.0,\n          68963.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.989286658416045,\n        \"min\": -38.98294,\n        \"max\": 59.360455,\n        \"num_unique_values\": 255,\n        \"samples\": [\n          40.715782,\n          13.963311,\n          39.517246\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"longitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 98.37190444736929,\n        \"min\": -123.78923,\n        \"max\": 145.03517,\n        \"num_unique_values\": 255,\n        \"samples\": [\n          -73.96677,\n          100.65349,\n          -119.896866\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"humidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.30430771047635,\n        \"min\": 5.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 74,\n        \"samples\": [\n          31.0,\n          47.6,\n          23.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temperature\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.575383467961075,\n        \"min\": 39.0,\n        \"max\": 368.0,\n        \"num_unique_values\": 59,\n        \"samples\": [\n          86.0,\n          88.0,\n          70.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pressure\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 81.49192521280047,\n        \"min\": 626.27,\n        \"max\": 1126.52,\n        \"num_unique_values\": 249,\n        \"samples\": [\n          929.94,\n          1009.21,\n          991.49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pm1.0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1602.1636301156284,\n        \"min\": 20.9,\n        \"max\": 5114.7,\n        \"num_unique_values\": 240,\n        \"samples\": [\n          63.2,\n          50.0,\n          3325.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pm2.5_alt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 107.26287147955352,\n        \"min\": 0.0,\n        \"max\": 1229.8,\n        \"num_unique_values\": 214,\n        \"samples\": [\n          44.1,\n          141.2,\n          353.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pm10.0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1582.6812349522488,\n        \"min\": 47.5,\n        \"max\": 5114.7,\n        \"num_unique_values\": 242,\n        \"samples\": [\n          101.1,\n          86.2,\n          3286.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pm1.0_cf_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2183.1045853571077,\n        \"min\": 32.0,\n        \"max\": 5987.3,\n        \"num_unique_values\": 244,\n        \"samples\": [\n          62.6,\n          76.0,\n          4792.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pm2.5_atm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1438.936586698391,\n        \"min\": 40.3,\n        \"max\": 3990.9,\n        \"num_unique_values\": 237,\n        \"samples\": [\n          115.8,\n          91.2,\n          281.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pm2.5_cf_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2158.645376346258,\n        \"min\": 48.0,\n        \"max\": 5987.3,\n        \"num_unique_values\": 239,\n        \"samples\": [\n          141.2,\n          124.3,\n          4994.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pm10.0_cf_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2161.5378976234933,\n        \"min\": 49.2,\n        \"max\": 5987.3,\n        \"num_unique_values\": 248,\n        \"samples\": [\n          141.1,\n          130.4,\n          495.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"anomaly\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": -1,\n        \"max\": -1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03462442833967644,\n        \"min\": -0.8174032581306395,\n        \"max\": -0.6886180652685274,\n        \"num_unique_values\": 255,\n        \"samples\": [\n          -0.6994270961177053\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weaknesses of the Batch Processing Approach for Real-Time Data Changes\n",
        "The batch processing approach described in the Jupyter notebook is well-suited for handling large datasets in a systematic manner, allowing for thorough cleaning, imputation, and anomaly detection. However, there are significant limitations when it comes to the adaptability and efficiency of this method, especially when dealing with data that changes in real time. Below are some critical weaknesses:\n",
        "\n",
        "1. Lag in Response Time\n",
        "Batch processing inherently involves processing data in large blocks at scheduled intervals. This results in a lag between data collection and data processing, making the approach less effective for applications that require real-time analysis or immediate action based on the latest data inputs. In environmental monitoring, for example, real-time data analysis can be crucial for issuing health advisories due to poor air quality.\n",
        "\n",
        "2. Scalability Issues with Frequent Updates\n",
        "As the data updates increase in frequency, the batch processing system may struggle to keep up without significant resources dedicated to handling these updates. If the data changes significantly between batches, the system might not capture transient anomalies or shifts in data trends effectively, potentially leading to missed detections or delayed responses.\n",
        "\n",
        "3. Inefficiency in Resource Usage\n",
        "Batch processes often require more computational resources because they handle large volumes of data at once. This can be inefficient, especially if only small parts of the dataset require updates or if the data contains a lot of redundancies. Continuous processing, on the other hand, can be more resource-efficient as it processes data incrementally.\n",
        "\n",
        "4. Difficulty Adapting to New Patterns\n",
        "The models used in batch processing are typically trained on historical data and might not adapt quickly to new or emerging patterns. This is particularly problematic for anomaly detection in environmental data, which can be influenced by sudden and unpredictable changes in environmental conditions. If the model cannot update its parameters in real-time or near-real-time, it may not perform well against newly evolving data trends.\n",
        "\n",
        "5. Potential for Data Drift\n",
        "Data drift refers to the change in the input data's distribution over time. In batch processing, there can be a significant delay between model updates, during which the data might drift, leading to model degradation. This can cause the model to make inaccurate predictions or fail to detect anomalies, as it no longer represents the current state of the environment accurately.\n",
        "\n",
        "6. Error Accumulation\n",
        "Errors in earlier stages of the batch processing pipeline can propagate and amplify by the time the data reaches the anomaly detection stage. Since data is processed in large chunks, identifying the source of errors or inconsistencies can be challenging, complicating the troubleshooting and adjustment processes.\n",
        "\n",
        "Conclusion\n",
        "While the batch processing approach offers a structured and comprehensive method for handling complex datasets, its application to real-time or near-real-time scenarios is limited. For environments where data is rapidly changing or where immediate data processing is crucial, a more dynamic approach such as stream processing might be necessary. Stream processing allows for continuous data ingestion and immediate analysis, which is more suitable for applications demanding quick responses and high adaptability."
      ],
      "metadata": {
        "id": "6unMvSIqJTm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this setup:\n",
        "\n",
        "The fit method is used to train the model on the data you expect to be normal.\n",
        "The predict method then labels new data points as normal or anomalous based on their isolation depth in the forest.\n",
        "This approach is quite flexible and allows for both initial training and subsequent anomaly detection on new batches of data."
      ],
      "metadata": {
        "id": "7qS6ock3pBf2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EGWn-ZJAJc1o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}