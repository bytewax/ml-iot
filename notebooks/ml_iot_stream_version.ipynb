{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Real-time processing of Air Quality Data for Anomaly detection\n",
        "\n",
        "\n",
        "To integrate our Python program with Bytewax for building a dataflow architecture, where input and deserialization are stateless and anomaly detection is stateful, we'll follow these steps:\n",
        "\n",
        "1. Set up the Bytewax Dataflow: Define the dataflow to ingest data, perform the deserialization, imputation, and pass the data through the anomaly detection which is stateful.\n",
        "2. Integrate Stateless Steps: These include reading input, deserializing data, and imputing missing values using KNN.\n",
        "3. Integrate Stateful Step: This will be your anomaly detection, which maintains state over the window of data it analyzes."
      ],
      "metadata": {
        "id": "tZjAyeUX0doV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f3mZtdsziK2",
        "outputId": "94718f6b-dcd5-46d9-fd0a-6f7460ae5858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bytewax==0.19 in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: scipy==1.13.0 in /usr/local/lib/python3.10/dist-packages (1.13.0)\n",
            "Requirement already satisfied: kafka-python==2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: jsonpickle>=3 in /usr/local/lib/python3.10/dist-packages (from bytewax==0.19) (3.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from bytewax==0.19) (4.11.0)\n",
            "Requirement already satisfied: prometheus-client>=0.18 in /usr/local/lib/python3.10/dist-packages (from bytewax==0.19) (0.20.0)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy==1.13.0) (1.25.2)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Collecting river\n",
            "  Downloading river-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (1.25.2)\n",
            "INFO: pip is looking at multiple versions of river to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading river-0.20.1.tar.gz (796 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.8/796.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading river-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from river) (1.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
            "Installing collected packages: river\n",
            "Successfully installed river-0.19.0\n",
            "Collecting scikit-learn==1.4.2\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2) (1.13.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2) (3.4.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install bytewax==0.19 python-dotenv scipy==1.13.0 kafka-python==2.0.2\n",
        "!pip install pandas==2.0.3 river\n",
        "!pip install scikit-learn==1.4.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bytewax.operators as op\n",
        "\n",
        "from bytewax.dataflow import Dataflow\n",
        "from bytewax.testing import TestingSource\n",
        "from bytewax.connectors.stdio import StdOutSink\n",
        "\n",
        "from bytewax.inputs import StatelessSourcePartition, DynamicSource\n",
        "\n",
        "\n",
        "import json\n",
        "from bytewax.testing import run_main\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "from river import anomaly\n",
        "from sklearn.impute import KNNImputer\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "g7fgubiLzkSi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Opening JSON file\n",
        "f = open('synthetic_data.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)\n"
      ],
      "metadata": {
        "id": "7JucX29j2s9r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To convert the `serialize` function into a Bytewax stream-equivalent format, we need to create a data source that behaves as a generator or a source of streaming data. Below, I will define two classes to model this behavior: one for partition-specific streaming data (`SerializedData`), and another to encapsulate the dynamic data generation across potentially multiple workers (`SerializedInput`).\n",
        "\n",
        "Step 1: Define `SerializedData` as a `StatelessSourcePartition`\n",
        "This class will act as a source partition that iterates over a dataset, serializing each entry according to the provided headers and fields.\n",
        "\n",
        "Step 2: Define `SerializedInput` as a `DynamicSource`\n",
        "This class encapsulates the partition management for the data source, ensuring that each worker in a distributed environment gets a proper instance of the source partition."
      ],
      "metadata": {
        "id": "pxQuEAAGFOka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SerializedData(StatelessSourcePartition):\n",
        "    \"\"\"\n",
        "    Emit serialized data directly for simplicity. This class will serialize\n",
        "    each entry in the 'data' list by mapping it to the corresponding 'fields'.\n",
        "    \"\"\"\n",
        "    def __init__(self, full_data):\n",
        "        self.fields = full_data['fields']\n",
        "        self.data_entries = full_data['data']\n",
        "        self.metadata = {k: v for k, v in full_data.items() if k not in ['fields', 'data']}\n",
        "        self._it = iter(self.data_entries)\n",
        "\n",
        "    def next_batch(self):\n",
        "        try:\n",
        "            data_entry = next(self._it)\n",
        "            # Map each entry in 'data' with the corresponding field in 'fields'\n",
        "            data_dict = dict(zip(self.fields, data_entry))\n",
        "            # Merge metadata with data_dict to form the complete record\n",
        "            complete_record = {**self.metadata, **{\"data\": data_dict}}\n",
        "            # Serialize the complete record\n",
        "            serialized = json.dumps(complete_record).encode('utf-8')\n",
        "            return [serialized]\n",
        "        except StopIteration:\n",
        "            raise StopIteration\n",
        "\n",
        "\n",
        "class SerializedInput(DynamicSource):\n",
        "    \"\"\"\n",
        "    Dynamic data source that partitions the input data among workers.\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.total_entries = len(data['data'])\n",
        "\n",
        "    def build(self, step_id, worker_index, worker_count):\n",
        "        # Calculate the slice of data each worker should handle\n",
        "        part_size = self.total_entries // worker_count\n",
        "        start = part_size * worker_index\n",
        "        end = start + part_size if worker_index != worker_count - 1 else self.total_entries\n",
        "\n",
        "        # Create a partition of the data for the specific worker\n",
        "        # Note: This partitions only the 'data' array. Metadata and fields are assumed\n",
        "        # to be common and small enough to be replicated across workers.\n",
        "        data_partition = {\n",
        "            \"api_version\": self.data['api_version'],\n",
        "            \"time_stamp\": self.data['time_stamp'],\n",
        "            \"data_time_stamp\": self.data['data_time_stamp'],\n",
        "            \"max_age\": self.data['max_age'],\n",
        "            \"firmware_default_version\": self.data['firmware_default_version'],\n",
        "            \"fields\": self.data['fields'],\n",
        "            \"data\": self.data['data'][start:end]\n",
        "        }\n",
        "\n",
        "        return SerializedData(data_partition)"
      ],
      "metadata": {
        "id": "AJ6kmNeNBWRT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Data Initialization: The `SerializedData` class now takes the entire data structure, keeps the metadata, and iterates over the data list. Each entry in data is mapped to the corresponding field specified in fields, combined with the metadata, serialized into a JSON string, and then encoded.\n",
        "\n",
        "* Integration into Dataflow: The class is used directly within a Bytewax dataflow as an input source, demonstrating how serialized data would be produced from the structured input.\n",
        "\n",
        "We can then deserialize the data with a simple function.\n"
      ],
      "metadata": {
        "id": "FLsbQnsnFzSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_deserialized_data(byte_data):\n",
        "    \"\"\"Deserialize byte data and prepare for stateful processing.\"\"\"\n",
        "    sensor_data = json.loads(byte_data.decode('utf-8'))['data']\n",
        "    key = str(sensor_data.get(\"sensor_index\", \"default\"))\n",
        "    return (key, sensor_data)"
      ],
      "metadata": {
        "id": "I9Zff7yIFy6K"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we can perform imputation of the missing values.\n",
        "\n",
        "Should KNN Imputation Be Stateful or Stateless?\n",
        "\n",
        "Stateless processing implies that each data item is processed independently without any need to remember past interactions. This is typically not the case with KNN imputation:\n",
        "\n",
        "* Stateful: The KNN algorithm typically benefits from \"remembering\" the dataset it uses to predict missing values because it bases its imputation on the k-nearest neighbors. Therefore, a stateful approach is often necessary if you need to continuously update the training dataset as new data arrives or if the dataset itself is too large to handle in a single batch efficiently.\n",
        "\n",
        "* Stateless: If the data chunks you process are independent or if the dataset can be managed in small batches without the need for continuity between batches, you could consider a stateless approach.\n",
        "\n",
        "For Bytewax, since it’s primarily built for stream processing, we might need to adjust your KNN implementation to fit into a stateful paradigm if our dataset is dynamically growing or if you need the model to adapt continuously as new data arrives.\n",
        "\n"
      ],
      "metadata": {
        "id": "UCaoulDYL4ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bytewax.inputs import StatefulSourcePartition, FixedPartitionedSource\n",
        "from bytewax.inputs import DynamicSource\n",
        "\n",
        "def impute_data_with_knn(batch):\n",
        "    df = pd.DataFrame(batch)\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'object':\n",
        "            try:\n",
        "                df[column] = pd.to_numeric(df[column])\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "    imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
        "    imputed_array = imputer.fit_transform(df[numeric_columns])\n",
        "    df[numeric_columns] = imputed_array\n",
        "    return df.to_dict(orient='records')\n",
        "\n",
        "def batch_accumulator(key, record):\n",
        "    \"\"\"Accumulate records into batches.\"\"\"\n",
        "    return [record]\n",
        "\n",
        "def batch_to_list(window_state):\n",
        "    \"\"\"Convert window state (list of lists) to a flat list of records.\"\"\"\n",
        "    flat_list = [item for sublist in window_state for item in sublist]\n",
        "    return flat_list"
      ],
      "metadata": {
        "id": "CYrFw0p0MZJh"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Integrate `KNNImputation` into the Bytewax Dataflow\n",
        "Next, use this class in your dataflow. Since we're focusing on processing rather than generating new data from a source, we'll integrate it directly after data deserialization:"
      ],
      "metadata": {
        "id": "ShJqVX7LMfYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bytewax.operators.window import TumblingWindow, WindowConfig\n",
        "from datetime import datetime, timedelta, timezone\n"
      ],
      "metadata": {
        "id": "e_Mc670jV7CN"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the dataflow\n",
        "flow = Dataflow(\"air-quality-flow\")\n",
        "inp = op.input(\"inp\", flow, SerializedInput(data))\n",
        "deserialize = op.map(\"deserialize\", inp, process_deserialized_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "window_config = TumblingWindow(\n",
        "    length=timedelta(seconds=10),\n",
        "    align_to=datetime(2023, 1, 1, tzinfo=timezone.utc)\n",
        ")\n",
        "\n",
        "\n",
        "# Window the data to create batches\n",
        "window = op.stateful_map(\"window\", deserialize, window_config, batch_accumulator, batch_to_list)\n",
        "\n",
        "# Impute data\n",
        "impute = op.map(\"impute\", window, impute_data_with_knn)\n",
        "\n",
        "# Output or further processing\n",
        "op.inspect(\"inspect_imputed\", impute)\n",
        "run_main(flow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "teQol0t4z2V8",
        "outputId": "c2869aa6-e527-4f23-ab55-4898bf96509c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "operator 'map' called incorrectly; see cause above",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bytewax/dataflow.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3185\u001b[0m         \"\"\"\n\u001b[0;32m-> 3186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3106\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3107\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'too many positional arguments'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: too many positional arguments",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-2f649e2248c3>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Window the data to create batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"window\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_accumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_to_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Impute data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bytewax/dataflow.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"operator {cls.__name__!r} called incorrectly; see cause above\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0mbound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: operator 'map' called incorrectly; see cause above"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7l3lH70BMLS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}