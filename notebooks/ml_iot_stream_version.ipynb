{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZjAyeUX0doV"
      },
      "source": [
        "## Real-time processing of Air Quality Data for Anomaly detection\n",
        "\n",
        "\n",
        "This notebook outlines a real-time data processing solution using the Bytewax framework for anomaly detection in air quality data. The transition from batch processing to stream processing can provide immediate analysis and response capabilities, crucial for dynamic data environments.\n",
        "\n",
        "### Stream Processing Pipeline Overview\n",
        "The data processing pipeline can be visualized as follows:\n",
        "\n",
        "```bash\n",
        "Data Ingestion -> Serialization -> Deserialization and Imputation -> Anomaly Detection -> Anomaly Filtering\n",
        "```\n",
        "\n",
        "### Detailed Code Walkthrough\n",
        "Below is a detailed explanation of each component of the pipeline, showing how each function contributes to the real-time data processing\n",
        "\n",
        "### Setup and Dependency Installation\n",
        "This section includes installation commands for necessary Python packages to handle data flow and machine learning operations.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f3mZtdsziK2",
        "outputId": "6521886e-fcd9-4dfd-b288-34127a91a6b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.8/796.8 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install bytewax==0.19 python-dotenv scipy==1.13.0 kafka-python==2.0.2 --q\n",
        "!pip install pandas==2.0.3 river --q\n",
        "!pip install scikit-learn==1.4.2 --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "g7fgubiLzkSi"
      },
      "outputs": [],
      "source": [
        "import bytewax.operators as op\n",
        "\n",
        "from bytewax.dataflow import Dataflow\n",
        "from bytewax.testing import TestingSource\n",
        "from bytewax.connectors.stdio import StdOutSink\n",
        "\n",
        "from bytewax.inputs import StatelessSourcePartition, DynamicSource\n",
        "\n",
        "\n",
        "import json\n",
        "from bytewax.testing import run_main\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "from river import anomaly\n",
        "from sklearn.impute import KNNImputer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from river import preprocessing\n",
        "from river import stats"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is fetched from an external URL and prepared for real-time processing through the defined classes `SerializedData` and `SerializedInput`."
      ],
      "metadata": {
        "id": "Uj9EL8VPK5Ud"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7JucX29j2s9r"
      },
      "outputs": [],
      "source": [
        "# Opening JSON file\n",
        "url = 'https://raw.githubusercontent.com/bytewax/ml-iot/main/data.json'\n",
        "\n",
        "resp = requests.get(url)\n",
        "data = json.loads(resp.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxQuEAAGFOka"
      },
      "source": [
        "\n",
        "To convert the `serialize` function into a Bytewax stream-equivalent format, we need to create a data source that behaves as a generator or a source of streaming data. Below, I will define two classes to model this behavior: one for partition-specific streaming data (`SerializedData`), and another to encapsulate the dynamic data generation across potentially multiple workers (`SerializedInput`).\n",
        "\n",
        "Step 1: Define `SerializedData` as a `StatelessSourcePartition`\n",
        "This class will act as a source partition that iterates over a dataset, serializing each entry according to the provided headers and fields.\n",
        "\n",
        "Step 2: Define `SerializedInput` as a `DynamicSource`\n",
        "This class encapsulates the partition management for the data source, ensuring that each worker in a distributed environment gets a proper instance of the source partition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AJ6kmNeNBWRT"
      },
      "outputs": [],
      "source": [
        "class SerializedData(StatelessSourcePartition):\n",
        "    \"\"\"\n",
        "    Emit serialized data directly for simplicity. This class will serialize\n",
        "    each entry in the 'data' list by mapping it to the corresponding 'fields'.\n",
        "    \"\"\"\n",
        "    def __init__(self, full_data):\n",
        "        self.fields = full_data['fields']\n",
        "        self.data_entries = full_data['data']\n",
        "        self.metadata = {k: v for k, v in full_data.items() if k not in ['fields', 'data']}\n",
        "        self._it = iter(self.data_entries)\n",
        "\n",
        "    def next_batch(self):\n",
        "        try:\n",
        "            data_entry = next(self._it)\n",
        "            # Map each entry in 'data' with the corresponding field in 'fields'\n",
        "            data_dict = dict(zip(self.fields, data_entry))\n",
        "            # Merge metadata with data_dict to form the complete record\n",
        "            complete_record = {**self.metadata, **{\"data\": data_dict}}\n",
        "            # Serialize the complete record\n",
        "            serialized = json.dumps(complete_record).encode('utf-8')\n",
        "            return [serialized]\n",
        "        except StopIteration:\n",
        "            raise StopIteration\n",
        "\n",
        "\n",
        "class SerializedInput(DynamicSource):\n",
        "    \"\"\"\n",
        "    Dynamic data source that partitions the input data among workers.\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.total_entries = len(data['data'])\n",
        "\n",
        "    def build(self, step_id, worker_index, worker_count):\n",
        "        # Calculate the slice of data each worker should handle\n",
        "        part_size = self.total_entries // worker_count\n",
        "        start = part_size * worker_index\n",
        "        end = start + part_size if worker_index != worker_count - 1 else self.total_entries\n",
        "\n",
        "        # Create a partition of the data for the specific worker\n",
        "        # Note: This partitions only the 'data' array. Metadata and fields are assumed\n",
        "        # to be common and small enough to be replicated across workers.\n",
        "        data_partition = {\n",
        "            \"api_version\": self.data['api_version'],\n",
        "            \"time_stamp\": self.data['time_stamp'],\n",
        "            \"data_time_stamp\": self.data['data_time_stamp'],\n",
        "            \"max_age\": self.data['max_age'],\n",
        "            \"firmware_default_version\": self.data['firmware_default_version'],\n",
        "            \"fields\": self.data['fields'],\n",
        "            \"data\": self.data['data'][start:end]\n",
        "        }\n",
        "\n",
        "        return SerializedData(data_partition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLsbQnsnFzSb"
      },
      "source": [
        "* Data Initialization: The `SerializedData` class now takes the entire data structure, keeps the metadata, and iterates over the data list. Each entry in data is mapped to the corresponding field specified in fields, combined with the metadata, serialized into a JSON string, and then encoded.\n",
        "\n",
        "* Integration into Dataflow: The class is used directly within a Bytewax dataflow as an input source, demonstrating how serialized data would be produced from the structured input.\n",
        "\n",
        "We can then deserialize the data with a simple function.\n",
        "\n",
        "In this function, we will also perform imputation of missing values, particularly, for the temperature, humidity, pressure attributes and PM attributes.\n",
        "\n",
        "Unlike the batch version, we will use the `river` library to perform imputation, this is due to the library's compatibility with stream processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "I9Zff7yIFy6K"
      },
      "outputs": [],
      "source": [
        "temp_imputer = preprocessing.StatImputer((\"temperature\", stats.Mean()))\n",
        "humidity_imputer = preprocessing.StatImputer((\"humidity\", stats.Mean()))\n",
        "pressure_imputer = preprocessing.StatImputer((\"pressure\", stats.Mean()))\n",
        "pm1_imputer = preprocessing.StatImputer((\"pm1.0_cf_1\", stats.Mean()))\n",
        "\n",
        "\n",
        "def process_and_impute_data(byte_data):\n",
        "    \"\"\"Deserialize byte data, impute missing values, and prepare for stateful processing.\"\"\"\n",
        "    # Deserialize the byte data\n",
        "    record = json.loads(byte_data.decode('utf-8'))\n",
        "    sensor_data = record['data']\n",
        "    key = str(sensor_data.get(\"sensor_index\", \"default\"))\n",
        "\n",
        "    # Impute missing values\n",
        "    for item in [temp_imputer, humidity_imputer, pressure_imputer, pm1_imputer]:\n",
        "      item.learn_one(sensor_data)\n",
        "      sensor_data = item.transform_one(sensor_data)\n",
        "    temp_imputer.learn_one(sensor_data)  # Update imputer with current data\n",
        "    sensor_data = temp_imputer.transform_one(sensor_data)  # Impute missing values\n",
        "\n",
        "    # Return the processed data with the key\n",
        "    return (key, sensor_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like in the batch example, we will use the `river` library's `HalfSpaceTrees` for detecting anomalies in streaming data.\n",
        "\n",
        "This class is integral for real-time anomaly detection. It maintains a model state across the data stream, continuously learning and scoring new data points."
      ],
      "metadata": {
        "id": "-bXC9Z1OLsR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnomalyDetector:\n",
        "    \"\"\"\n",
        "    Anomaly detector using HalfSpaceTrees from River library\n",
        "\n",
        "    This class is used to detect anomalies in the data using online ML models\n",
        "    with the River library\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_trees=10, height=8, window_size=72, seed=11):\n",
        "        \"\"\"\n",
        "        Initialize the anomaly detector\n",
        "        \"\"\"\n",
        "        self.detector = anomaly.HalfSpaceTrees(\n",
        "            n_trees=n_trees,\n",
        "            height=height,\n",
        "            window_size=window_size,\n",
        "            limits={'pm1.0_cf_1': (0.0, 1200)},  # Ensure these limits make sense for your data\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "    def update(self, data):\n",
        "        \"\"\"\n",
        "        Update the anomaly detector with new data\n",
        "        \"\"\"\n",
        "        # Check if 'pm1.0_cf_1' is not None and is a floatable type\n",
        "        if data.get('pm1.0_cf_1') is not None:\n",
        "            try:\n",
        "                value = float(data['pm1.0_cf_1'])\n",
        "                score = self.detector.score_one({'pm1.0_cf_1': value})\n",
        "                self.detector.learn_one({'pm1.0_cf_1': value})\n",
        "                data['anomaly_score'] = score\n",
        "            except ValueError:\n",
        "                print(f\"Skipping entry, invalid data for pm1.0_cf_1: {data['pm1.0_cf_1']}\")\n",
        "        else:\n",
        "            data['anomaly_score'] = None\n",
        "        return data\n",
        "\n",
        "\n",
        "# Initialize the anomaly detector\n",
        "anomaly_detector = AnomalyDetector()\n",
        "\n",
        "def detect_anomalies(data_tuple):\n",
        "    \"\"\"Detect anomalies in sensor data.\"\"\"\n",
        "    key, sensor_data = data_tuple\n",
        "    sensor_data = anomaly_detector.update(sensor_data)\n",
        "    return (key, sensor_data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lu3Fq9bL9rfS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only anomalies exceeding a specified threshold are passed forward for alerting or further analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "cpi-EQmGMs3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_high_anomaly(data_tuple):\n",
        "    \"\"\"Filter entries with high anomaly scores.\"\"\"\n",
        "    key, data = data_tuple\n",
        "    # Check if 'anomaly_score' is greater than 0.7\n",
        "    return data.get('anomaly_score', 0) > 0.7\n"
      ],
      "metadata": {
        "id": "79ZaSUmp-5d9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/bytewax/ml-iot/blob/main/flow.png?raw=true)"
      ],
      "metadata": {
        "id": "OHRbG1dyM7FA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "teQol0t4z2V8"
      },
      "outputs": [],
      "source": [
        "# Setup the dataflow\n",
        "flow = Dataflow(\"air-quality-flow\")\n",
        "inp = op.input(\"inp\", flow, SerializedInput(data))\n",
        "impute_deserialize = op.map(\"impute_deserialize\", inp, process_and_impute_data)\n",
        "\n",
        "\n",
        "# Add anomaly detection to the dataflow\n",
        "detect_anomalies_step = op.map(\"detect_anomalies\", impute_deserialize, detect_anomalies)\n",
        "\n",
        "# Detect anomalies within threshold\n",
        "filter_anomalies = op.filter(\"filter_high_anomalies\", detect_anomalies_step, filter_high_anomaly)\n",
        "\n",
        "\n",
        "# Output or further processing\n",
        "op.inspect(\"inspect_filtered_anomalies\", filter_anomalies)\n",
        "\n",
        "\n",
        "run_main(flow)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison with Batch Approach\n",
        "1. Real-time Adaptability: Unlike the batch processing approach, this stream processing framework is designed to handle data in real-time, which significantly reduces the latency between data acquisition and processing. This is crucial for applications where timely data analysis can lead to immediate actionable insights.\n",
        "\n",
        "2. Scalability and Efficiency: The stream processing model is inherently more scalable and efficient for data that frequently updates. It processes data incrementally, avoiding the overhead associated with batch processing large datasets at once.\n",
        "\n",
        "3. Continuous Learning and Adaptation: The real-time model continuously updates its parameters with incoming data, making it more adaptable to new patterns or changes in data trends. This contrasts with batch processing, where models might not adapt quickly to new data between batch runs.\n",
        "\n",
        "4. Error Handling and Data Drift: Stream processing allows for immediate error detection and handling, which can prevent error propagation that is often seen in batch processes. Additionally, it handles data drift more effectively by continuously adapting the model to the incoming data stream.\n",
        "\n",
        "## Conclusion\n",
        "The Bytewax-based real-time data processing approach provides a robust solution for handling air quality data in environments that require immediate analysis and response. This method is superior in scenarios where data integrity, timeliness, and adaptability are crucial, making it an ideal choice over batch processing for dynamic data applications.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_FVgxjNqMwG9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F3awiUb-Mz1a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}